"""
Stage 5: Report Generation

Generates comprehensive evaluation reports from the pipeline results.

Report includes:
- Executive summary with key metrics
- Quality level analysis (perfect/medium/low)
- Structural vs Grounding breakdown
- Individual assertion results
- Lessons learned and recommendations

Usage:
    python -m pipeline.report_generation
    python -m pipeline.report_generation --evaluation docs/pipeline_output/evaluation_results.json
"""

import json
import argparse
from datetime import datetime
from typing import List, Dict, Optional

from .config import (
    SCENARIOS_FILE,
    ASSERTIONS_FILE,
    PLANS_FILE,
    EVALUATION_FILE,
    REPORT_FILE,
    save_json,
    load_json
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Report Templates
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REPORT_TEMPLATE = """# Two-Layer Assertion Evaluation Report

> **Generated:** {generated_at}  
> **Framework:** Two-Layer Assertion Framework v2.0 (Structural S1-S10 + Grounding G1-G5)

---

## Executive Summary

{executive_summary}

---

## Key Metrics

| Metric | Value |
|--------|-------|
| Total Plans Evaluated | {total_plans} |
| Total Assertions | {total_assertions} |
| Overall Structural Score | {overall_structural}% |
| Overall Grounding Score | {overall_grounding}% |
| Pass Rate | {pass_rate}% |

---

## Results by Quality Level

{quality_level_section}

---

## Two-Layer Framework Analysis

### Structural Assertions (S1-S10)
*Question: "Does the plan HAVE X?" (Checks PRESENCE)*

{structural_analysis}

### Grounding Assertions (G1-G5)
*Question: "Is X CORRECT?" (Checks ACCURACY)*

{grounding_analysis}

---

## Verdict Distribution

{verdict_section}

---

## Detailed Results

{detailed_results}

---

## Insights & Recommendations

{insights_section}

---

## Appendix: Two-Layer Framework Reference

### Structural Patterns (S1-S10)

| ID | Pattern | Purpose |
|----|---------|---------|
| S1 | Explicit Meeting Details | Has date, time, timezone, attendees |
| S2 | Timeline Alignment | Has timeline working back from meeting |
| S3 | Ownership Assignment | Has named owners (not "someone") |
| S4 | Artifact Specification | Lists specific files/documents |
| S5 | Date Specification | States completion dates for tasks |
| S6 | Blocker Identification | Identifies dependencies and blockers |
| S7 | Source Traceability | Links tasks to source entities |
| S8 | Communication Channels | Mentions communication methods |
| S9 | Grounding Meta-Check | Passes when G1-G5 all pass |
| S10 | Priority Assignment | Has priority levels for tasks |

### Grounding Patterns (G1-G5)

| ID | Pattern | Source Field |
|----|---------|--------------|
| G1 | People Grounding | source.attendees |
| G2 | Temporal Grounding | source.meeting_date/time |
| G3 | Artifact Grounding | source.files |
| G4 | Topic Grounding | source.topics |
| G5 | Hallucination Check | All source fields |

---

*Report generated by the Two-Layer Assertion Pipeline*
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Report Generation Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_executive_summary(evaluation_data: Dict) -> str:
    """Generate executive summary section."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    by_verdict = summary.get("by_verdict", {})
    
    # Calculate key insights
    total_pass = by_verdict.get("pass", 0)
    total_fail = sum(v for k, v in by_verdict.items() if k != "pass")
    
    lines = []
    lines.append(f"This report evaluates **{summary.get('total_plans', 0)} workback plans** across three quality levels using the Two-Layer Assertion Framework.")
    lines.append("")
    
    # Perfect quality insight
    if "perfect" in by_quality:
        perfect = by_quality["perfect"]
        lines.append(f"**Perfect Quality Plans:** Achieved {perfect['avg_structural']*100:.0f}% structural and {perfect['avg_grounding']*100:.0f}% grounding scores, {'meeting' if perfect['avg_structural'] >= 0.9 and perfect['avg_grounding'] >= 0.9 else 'approaching'} expected targets.")
    
    # Medium quality insight
    if "medium" in by_quality:
        medium = by_quality["medium"]
        lines.append(f"**Medium Quality Plans:** Achieved {medium['avg_structural']*100:.0f}% structural and {medium['avg_grounding']*100:.0f}% grounding scores, demonstrating deliberate quality degradation.")
    
    # Low quality insight
    if "low" in by_quality:
        low = by_quality["low"]
        lines.append(f"**Low Quality Plans:** Achieved {low['avg_structural']*100:.0f}% structural and {low['avg_grounding']*100:.0f}% grounding scores, confirming detection of poor quality.")
    
    lines.append("")
    lines.append(f"**Overall:** The framework correctly differentiated quality levels, with {total_pass} plans passing and {total_fail} failing as expected.")
    
    return "\n".join(lines)


def generate_quality_level_section(evaluation_data: Dict) -> str:
    """Generate quality level breakdown section."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    results = evaluation_data.get("results", [])
    
    lines = []
    
    for quality in ["perfect", "medium", "low"]:
        if quality not in by_quality:
            continue
        
        stats = by_quality[quality]
        quality_results = [r for r in results if r.get("quality_level") == quality]
        
        lines.append(f"### {quality.capitalize()} Quality")
        lines.append("")
        lines.append(f"**Target:** {'100% S, 100% G' if quality == 'perfect' else '80% S, 60% G' if quality == 'medium' else '40% S, 20% G'}")
        lines.append(f"**Actual:** {stats['avg_structural']*100:.0f}% Structural, {stats['avg_grounding']*100:.0f}% Grounding")
        lines.append(f"**Plans Evaluated:** {stats['count']}")
        lines.append("")
        
        # Show verdicts for this quality level
        verdicts = {}
        for r in quality_results:
            v = r.get("overall_verdict", "unknown")
            verdicts[v] = verdicts.get(v, 0) + 1
        
        lines.append("| Verdict | Count |")
        lines.append("|---------|-------|")
        for v, count in verdicts.items():
            emoji = "âœ…" if v == "pass" else "âŒ"
            lines.append(f"| {emoji} {v} | {count} |")
        lines.append("")
    
    return "\n".join(lines)


def generate_structural_analysis(evaluation_data: Dict) -> str:
    """Generate structural assertions analysis."""
    results = evaluation_data.get("results", [])
    
    # Aggregate structural results across all evaluations
    pattern_stats = {}
    
    for result in results:
        for sr in result.get("structural_results", []):
            aid = sr.get("assertion_id", "Unknown")
            if aid not in pattern_stats:
                pattern_stats[aid] = {"passed": 0, "failed": 0}
            
            if sr.get("passed"):
                pattern_stats[aid]["passed"] += 1
            else:
                pattern_stats[aid]["failed"] += 1
    
    if not pattern_stats:
        return "*No structural assertions evaluated.*"
    
    lines = []
    lines.append("| Assertion | Pass | Fail | Rate |")
    lines.append("|-----------|------|------|------|")
    
    for aid in sorted(pattern_stats.keys()):
        stats = pattern_stats[aid]
        total = stats["passed"] + stats["failed"]
        rate = stats["passed"] / total * 100 if total > 0 else 0
        emoji = "âœ…" if rate >= 80 else "âš ï¸" if rate >= 50 else "âŒ"
        lines.append(f"| {aid} | {stats['passed']} | {stats['failed']} | {emoji} {rate:.0f}% |")
    
    return "\n".join(lines)


def generate_grounding_analysis(evaluation_data: Dict) -> str:
    """Generate grounding assertions analysis."""
    results = evaluation_data.get("results", [])
    
    # Aggregate grounding results
    pattern_stats = {}
    hallucinations = []
    
    for result in results:
        for gr in result.get("grounding_results", []):
            aid = gr.get("assertion_id", "Unknown")
            if aid not in pattern_stats:
                pattern_stats[aid] = {"passed": 0, "failed": 0}
            
            if gr.get("passed"):
                pattern_stats[aid]["passed"] += 1
            else:
                pattern_stats[aid]["failed"] += 1
                # Track hallucinations
                for span in gr.get("supporting_spans", []):
                    if span.get("type") == "mismatch":
                        hallucinations.append({
                            "quality": result.get("quality_level"),
                            "assertion": aid,
                            "value": span.get("text", "")
                        })
    
    if not pattern_stats:
        return "*No grounding assertions evaluated.*"
    
    lines = []
    lines.append("| Assertion | Pass | Fail | Rate |")
    lines.append("|-----------|------|------|------|")
    
    for aid in sorted(pattern_stats.keys()):
        stats = pattern_stats[aid]
        total = stats["passed"] + stats["failed"]
        rate = stats["passed"] / total * 100 if total > 0 else 0
        emoji = "âœ…" if rate >= 80 else "âš ï¸" if rate >= 50 else "âŒ"
        lines.append(f"| {aid} | {stats['passed']} | {stats['failed']} | {emoji} {rate:.0f}% |")
    
    # Show sample hallucinations
    if hallucinations:
        lines.append("")
        lines.append("**Sample Hallucinations Detected:**")
        for h in hallucinations[:5]:  # Show up to 5
            lines.append(f"- [{h['quality']}] {h['assertion']}: `{h['value'][:50]}...`")
    
    return "\n".join(lines)


def generate_verdict_section(evaluation_data: Dict) -> str:
    """Generate verdict distribution section."""
    summary = evaluation_data.get("summary", {})
    by_verdict = summary.get("by_verdict", {})
    
    total = sum(by_verdict.values())
    if total == 0:
        return "*No verdicts recorded.*"
    
    lines = []
    lines.append("| Verdict | Count | Percentage |")
    lines.append("|---------|-------|------------|")
    
    verdict_emoji = {
        "pass": "âœ…",
        "fail_structure": "ðŸ”§",
        "fail_grounding": "âš ï¸",
        "fail_both": "âŒ"
    }
    
    for verdict in ["pass", "fail_structure", "fail_grounding", "fail_both"]:
        count = by_verdict.get(verdict, 0)
        pct = count / total * 100 if total > 0 else 0
        emoji = verdict_emoji.get(verdict, "â“")
        lines.append(f"| {emoji} {verdict} | {count} | {pct:.0f}% |")
    
    # Add interpretation
    lines.append("")
    lines.append("**Interpretation:**")
    lines.append("- âœ… **pass**: Plan has good structure AND is factually accurate")
    lines.append("- ðŸ”§ **fail_structure**: Plan is missing required elements")
    lines.append("- âš ï¸ **fail_grounding**: Plan has good structure but contains hallucinations")
    lines.append("- âŒ **fail_both**: Plan is both incomplete and inaccurate")
    
    return "\n".join(lines)


def generate_detailed_results(evaluation_data: Dict) -> str:
    """Generate detailed results for each plan."""
    results = evaluation_data.get("results", [])
    
    if not results:
        return "*No detailed results available.*"
    
    lines = []
    
    for i, result in enumerate(results[:10]):  # Limit to first 10 for readability
        quality = result.get("quality_level", "unknown")
        scenario = result.get("scenario_id", "unknown")
        s_score = result.get("structural_score", 0) * 100
        g_score = result.get("grounding_score", 0) * 100
        verdict = result.get("overall_verdict", "unknown")
        
        verdict_emoji = {
            "pass": "âœ…",
            "fail_structure": "ðŸ”§",
            "fail_grounding": "âš ï¸",
            "fail_both": "âŒ"
        }.get(verdict, "â“")
        
        lines.append(f"### Plan {i+1}: {scenario} ({quality.capitalize()})")
        lines.append("")
        lines.append(f"- **Structural Score:** {s_score:.0f}%")
        lines.append(f"- **Grounding Score:** {g_score:.0f}%")
        lines.append(f"- **Verdict:** {verdict_emoji} {verdict}")
        lines.append("")
        
        # Show failed assertions
        failed_structural = [r for r in result.get("structural_results", []) if not r.get("passed")]
        failed_grounding = [r for r in result.get("grounding_results", []) if not r.get("passed")]
        
        if failed_structural:
            lines.append("**Failed Structural Assertions:**")
            for fs in failed_structural[:3]:
                lines.append(f"- {fs.get('assertion_id')}: {fs.get('explanation', 'No explanation')[:80]}")
            lines.append("")
        
        if failed_grounding:
            lines.append("**Failed Grounding Assertions:**")
            for fg in failed_grounding[:3]:
                lines.append(f"- {fg.get('assertion_id')}: {fg.get('explanation', 'No explanation')[:80]}")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    if len(results) > 10:
        lines.append(f"*... and {len(results) - 10} more results (see JSON output for complete data)*")
    
    return "\n".join(lines)


def generate_insights_section(evaluation_data: Dict) -> str:
    """Generate insights and recommendations."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    
    lines = []
    
    # Framework effectiveness
    lines.append("### Framework Effectiveness")
    lines.append("")
    
    # Check if quality levels were properly differentiated
    perfect_s = by_quality.get("perfect", {}).get("avg_structural", 0)
    medium_s = by_quality.get("medium", {}).get("avg_structural", 0)
    low_s = by_quality.get("low", {}).get("avg_structural", 0)
    
    if perfect_s > medium_s > low_s:
        lines.append("âœ… **Quality Differentiation:** The framework correctly ranked quality levels (Perfect > Medium > Low)")
    else:
        lines.append("âš ï¸ **Quality Differentiation:** Quality levels may not be clearly differentiated")
    
    lines.append("")
    
    # Structural vs Grounding balance
    lines.append("### Structural vs Grounding Insights")
    lines.append("")
    
    for quality in ["perfect", "medium", "low"]:
        if quality not in by_quality:
            continue
        stats = by_quality[quality]
        s_diff = abs(stats["avg_structural"] - stats["avg_grounding"])
        if s_diff > 0.2:
            lines.append(f"- **{quality.capitalize()}:** {s_diff*100:.0f}% gap between structural ({stats['avg_structural']*100:.0f}%) and grounding ({stats['avg_grounding']*100:.0f}%) scores")
    
    lines.append("")
    
    # Recommendations
    lines.append("### Recommendations")
    lines.append("")
    lines.append("1. **Structural assertions** should focus on PRESENCE checking only")
    lines.append("2. **Grounding assertions** should always reference specific source fields")
    lines.append("3. **S9 (Grounding Meta-Check)** should be computed as AND(G1-G5)")
    lines.append("4. Plans with high structural but low grounding scores indicate hallucinations")
    lines.append("5. Plans with low structural scores need more complete content")
    
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_report(evaluation_data: Dict) -> str:
    """Generate the complete evaluation report."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    results = evaluation_data.get("results", [])
    
    # Calculate overall metrics
    total_structural = sum(len(r.get("structural_results", [])) for r in results)
    total_grounding = sum(len(r.get("grounding_results", [])) for r in results)
    
    all_s_scores = [r.get("structural_score", 0) for r in results]
    all_g_scores = [r.get("grounding_score", 0) for r in results]
    
    overall_s = sum(all_s_scores) / len(all_s_scores) * 100 if all_s_scores else 0
    overall_g = sum(all_g_scores) / len(all_g_scores) * 100 if all_g_scores else 0
    
    pass_count = summary.get("by_verdict", {}).get("pass", 0)
    pass_rate = pass_count / len(results) * 100 if results else 0
    
    # Generate report sections
    report = REPORT_TEMPLATE.format(
        generated_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        total_plans=len(results),
        total_assertions=total_structural + total_grounding,
        overall_structural=f"{overall_s:.0f}",
        overall_grounding=f"{overall_g:.0f}",
        pass_rate=f"{pass_rate:.0f}",
        executive_summary=generate_executive_summary(evaluation_data),
        quality_level_section=generate_quality_level_section(evaluation_data),
        structural_analysis=generate_structural_analysis(evaluation_data),
        grounding_analysis=generate_grounding_analysis(evaluation_data),
        verdict_section=generate_verdict_section(evaluation_data),
        detailed_results=generate_detailed_results(evaluation_data),
        insights_section=generate_insights_section(evaluation_data)
    )
    
    return report


def main():
    """Main entry point for report generation."""
    parser = argparse.ArgumentParser(description="Stage 5: Report Generation")
    parser.add_argument("--evaluation", type=str, default=EVALUATION_FILE, help="Input evaluation file")
    parser.add_argument("--output", type=str, default=REPORT_FILE, help="Output report file (markdown)")
    parser.add_argument("--json", action="store_true", help="Also output JSON summary")
    args = parser.parse_args()
    
    print("\nðŸ“ˆ Stage 5: Report Generation")
    print("=" * 60)
    
    # Load evaluation data
    evaluation_data = load_json(args.evaluation)
    print(f"  Loaded evaluation data from {args.evaluation}")
    
    # Generate report
    report = generate_report(evaluation_data)
    
    # Save report
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"  ðŸ’¾ Saved report: {args.output}")
    
    # Optionally save JSON summary
    if args.json:
        json_output = args.output.replace(".md", "_summary.json")
        summary = {
            "generated_at": datetime.now().isoformat(),
            "source": args.evaluation,
            **evaluation_data.get("summary", {})
        }
        save_json(summary, json_output)
    
    print(f"\nâœ… Stage 5 Complete")
    print(f"   Report: {args.output}")
    
    return report


if __name__ == "__main__":
    main()
