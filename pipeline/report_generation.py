"""
Stage 5: Report Generation

Generates comprehensive evaluation reports from the pipeline results.

Report includes:
- Executive summary with key metrics
- Quality level analysis (perfect/medium/low)
- Structural vs Grounding breakdown
- Individual assertion results
- Lessons learned and recommendations

Usage:
    python -m pipeline.report_generation
    python -m pipeline.report_generation --evaluation docs/pipeline_output/evaluation_results.json
"""

import json
import argparse
from datetime import datetime
from typing import List, Dict, Optional

from .config import (
    SCENARIOS_FILE,
    ASSERTIONS_FILE,
    PLANS_FILE,
    EVALUATION_FILE,
    REPORT_FILE,
    save_json,
    load_json
)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Report Templates
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

REPORT_TEMPLATE = """# Two-Layer Assertion Evaluation Report

> **Generated:** {generated_at}  
> **Framework:** Two-Layer Assertion Framework v2.0 (Structural S1-S10 + Grounding G1-G5)

---

## Executive Summary

{executive_summary}

---

## Key Metrics

| Metric | Value |
|--------|-------|
| Total Scenarios | {total_scenarios} |
| Total Plans Evaluated | {total_plans} |
| Total Assertions | {total_assertions} |
| Overall Structural Score | {overall_structural}% |
| Overall Grounding Score | {overall_grounding}% |
| Pass Rate | {pass_rate}% |

---

## Scenarios

{scenarios_section}

---

## Assertions

{assertions_section}

---

## Plans

{plans_section}

---

## Evaluation Results by Quality Level

{quality_level_section}

---

## Two-Layer Framework Analysis

### Structural Assertions (S1-S10)
*Question: "Does the plan HAVE X?" (Checks PRESENCE)*

{structural_analysis}

### Grounding Assertions (G1-G5)
*Question: "Is X CORRECT?" (Checks ACCURACY)*

{grounding_analysis}

---

## Verdict Distribution

{verdict_section}

---

## Detailed Results

{detailed_results}

---

## Insights & Recommendations

{insights_section}

---

## Appendix: Two-Layer Framework Reference

### Structural Patterns (S1-S10)

| ID | Pattern | Purpose |
|----|---------|---------|
| S1 | Explicit Meeting Details | Has date, time, timezone, attendees |
| S2 | Timeline Alignment | Has timeline working back from meeting |
| S3 | Ownership Assignment | Has named owners (not "someone") |
| S4 | Artifact Specification | Lists specific files/documents |
| S5 | Date Specification | States completion dates for tasks |
| S6 | Blocker Identification | Identifies dependencies and blockers |
| S7 | Source Traceability | Links tasks to source entities |
| S8 | Communication Channels | Mentions communication methods |
| S9 | Grounding Meta-Check | Passes when G1-G5 all pass |
| S10 | Priority Assignment | Has priority levels for tasks |

### Grounding Patterns (G1-G5)

| ID | Pattern | Source Field |
|----|---------|--------------|
| G1 | People Grounding | source.attendees |
| G2 | Temporal Grounding | source.meeting_date/time |
| G3 | Artifact Grounding | source.files |
| G4 | Topic Grounding | source.topics |
| G5 | Hallucination Check | All source fields |

---

*Report generated by the Two-Layer Assertion Pipeline*
"""


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Report Generation Functions
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_scenarios_section(scenarios_data: Dict) -> str:
    """Generate scenarios section showing all input scenarios."""
    scenarios = scenarios_data.get("scenarios", [])
    
    if not scenarios:
        return "*No scenarios loaded.*"
    
    lines = []
    
    for scenario in scenarios:
        sid = scenario.get("id", "unknown")
        title = scenario.get("title", "Untitled")
        date = scenario.get("date", "N/A")
        time = scenario.get("time", "N/A")
        timezone = scenario.get("timezone", "N/A")
        organizer = scenario.get("organizer", "N/A")
        attendees = scenario.get("attendees", [])
        context = scenario.get("context", "")
        artifacts = scenario.get("artifacts", [])
        user_prompt = scenario.get("user_prompt", "")
        
        lines.append(f"### {sid}: {title}")
        lines.append("")
        lines.append(f"**Meeting Details:**")
        lines.append(f"- üìÖ **Date:** {date}")
        lines.append(f"- ‚è∞ **Time:** {time} ({timezone})")
        lines.append(f"- üë§ **Organizer:** {organizer}")
        lines.append(f"- üë• **Attendees:** {', '.join(attendees)}")
        lines.append("")
        
        if context:
            lines.append(f"**Context:** {context[:200]}{'...' if len(context) > 200 else ''}")
            lines.append("")
        
        if artifacts:
            lines.append(f"**Artifacts:** {', '.join(artifacts[:5])}")
            lines.append("")
        
        if user_prompt:
            lines.append(f"**User Prompt:** _{user_prompt}_")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    return "\n".join(lines)


def generate_assertions_section(assertions_data: Dict) -> str:
    """Generate assertions section showing all generated assertions."""
    assertions_list = assertions_data.get("assertions", [])
    
    if not assertions_list:
        return "*No assertions loaded.*"
    
    lines = []
    
    for assertion_set in assertions_list:
        scenario_id = assertion_set.get("scenario_id", "unknown")
        structural = assertion_set.get("structural", [])
        grounding = assertion_set.get("grounding", [])
        
        lines.append(f"### {scenario_id}")
        lines.append("")
        
        # Structural assertions table
        lines.append("**Structural Assertions (S1-S10):** _Check PRESENCE_")
        lines.append("")
        lines.append("| ID | Pattern | Assertion | Level |")
        lines.append("|----|---------|-----------|-------|")
        for s in structural:
            aid = s.get("id", "")
            pattern = s.get("pattern_id", "")
            text = s.get("text", "")[:60] + ("..." if len(s.get("text", "")) > 60 else "")
            level = s.get("level", "expected")
            lines.append(f"| {aid} | {pattern} | {text} | {level} |")
        lines.append("")
        
        # Grounding assertions table
        lines.append("**Grounding Assertions (G1-G5):** _Check ACCURACY_")
        lines.append("")
        lines.append("| ID | Pattern | Assertion | Source Field |")
        lines.append("|----|---------|-----------|--------------|")
        for g in grounding:
            gid = g.get("id", "")
            pattern = g.get("pattern_id", "")
            text = g.get("text", "")[:60] + ("..." if len(g.get("text", "")) > 60 else "")
            source = g.get("source_field", "")
            lines.append(f"| {gid} | {pattern} | {text} | {source} |")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    return "\n".join(lines)


def generate_plans_section(plans_data: Dict) -> str:
    """Generate plans section showing all generated plans."""
    plans = plans_data.get("plans", [])
    
    if not plans:
        return "*No plans loaded.*"
    
    lines = []
    
    # Group by scenario
    plans_by_scenario = {}
    for plan in plans:
        sid = plan.get("scenario_id", "unknown")
        if sid not in plans_by_scenario:
            plans_by_scenario[sid] = []
        plans_by_scenario[sid].append(plan)
    
    for scenario_id, scenario_plans in plans_by_scenario.items():
        lines.append(f"### {scenario_id}")
        lines.append("")
        
        for plan in scenario_plans:
            quality = plan.get("quality_level", "unknown")
            intended_s = plan.get("intended_structural_score", 0) * 100
            intended_g = plan.get("intended_grounding_score", 0) * 100
            issues = plan.get("deliberate_issues", [])
            content = plan.get("content", "")
            
            quality_emoji = {"perfect": "üåü", "medium": "‚ö°", "low": "‚ö†Ô∏è"}.get(quality, "‚ùì")
            
            lines.append(f"#### {quality_emoji} {quality.capitalize()} Quality Plan")
            lines.append("")
            lines.append(f"**Intended Scores:** Structural {intended_s:.0f}%, Grounding {intended_g:.0f}%")
            
            if issues:
                lines.append(f"**Deliberate Issues:** {', '.join(issues[:3])}")
            
            lines.append("")
            lines.append("<details>")
            lines.append("<summary>üìã View Plan Content</summary>")
            lines.append("")
            lines.append("```")
            # Truncate very long content
            if len(content) > 1500:
                lines.append(content[:1500] + "\n... (truncated)")
            else:
                lines.append(content)
            lines.append("```")
            lines.append("")
            lines.append("</details>")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    return "\n".join(lines)


def generate_executive_summary(evaluation_data: Dict) -> str:
    """Generate executive summary section."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    by_verdict = summary.get("by_verdict", {})
    
    # Calculate key insights
    total_pass = by_verdict.get("pass", 0)
    total_fail = sum(v for k, v in by_verdict.items() if k != "pass")
    
    lines = []
    lines.append(f"This report evaluates **{summary.get('total_plans', 0)} workback plans** across three quality levels using the Two-Layer Assertion Framework.")
    lines.append("")
    
    # Perfect quality insight
    if "perfect" in by_quality:
        perfect = by_quality["perfect"]
        lines.append(f"**Perfect Quality Plans:** Achieved {perfect['avg_structural']*100:.0f}% structural and {perfect['avg_grounding']*100:.0f}% grounding scores, {'meeting' if perfect['avg_structural'] >= 0.9 and perfect['avg_grounding'] >= 0.9 else 'approaching'} expected targets.")
    
    # Medium quality insight
    if "medium" in by_quality:
        medium = by_quality["medium"]
        lines.append(f"**Medium Quality Plans:** Achieved {medium['avg_structural']*100:.0f}% structural and {medium['avg_grounding']*100:.0f}% grounding scores, demonstrating deliberate quality degradation.")
    
    # Low quality insight
    if "low" in by_quality:
        low = by_quality["low"]
        lines.append(f"**Low Quality Plans:** Achieved {low['avg_structural']*100:.0f}% structural and {low['avg_grounding']*100:.0f}% grounding scores, confirming detection of poor quality.")
    
    lines.append("")
    lines.append(f"**Overall:** The framework correctly differentiated quality levels, with {total_pass} plans passing and {total_fail} failing as expected.")
    
    return "\n".join(lines)


def generate_quality_level_section(evaluation_data: Dict) -> str:
    """Generate quality level breakdown section."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    results = evaluation_data.get("results", [])
    
    lines = []
    
    for quality in ["perfect", "medium", "low"]:
        if quality not in by_quality:
            continue
        
        stats = by_quality[quality]
        quality_results = [r for r in results if r.get("quality_level") == quality]
        
        lines.append(f"### {quality.capitalize()} Quality")
        lines.append("")
        lines.append(f"**Target:** {'100% S, 100% G' if quality == 'perfect' else '80% S, 60% G' if quality == 'medium' else '40% S, 20% G'}")
        lines.append(f"**Actual:** {stats['avg_structural']*100:.0f}% Structural, {stats['avg_grounding']*100:.0f}% Grounding")
        lines.append(f"**Plans Evaluated:** {stats['count']}")
        lines.append("")
        
        # Show verdicts for this quality level
        verdicts = {}
        for r in quality_results:
            v = r.get("overall_verdict", "unknown")
            verdicts[v] = verdicts.get(v, 0) + 1
        
        lines.append("| Verdict | Count |")
        lines.append("|---------|-------|")
        for v, count in verdicts.items():
            emoji = "‚úÖ" if v == "pass" else "‚ùå"
            lines.append(f"| {emoji} {v} | {count} |")
        lines.append("")
    
    return "\n".join(lines)


def generate_structural_analysis(evaluation_data: Dict) -> str:
    """Generate structural assertions analysis."""
    results = evaluation_data.get("results", [])
    
    # Aggregate structural results across all evaluations
    pattern_stats = {}
    
    for result in results:
        for sr in result.get("structural_results", []):
            aid = sr.get("assertion_id", "Unknown")
            if aid not in pattern_stats:
                pattern_stats[aid] = {"passed": 0, "failed": 0}
            
            if sr.get("passed"):
                pattern_stats[aid]["passed"] += 1
            else:
                pattern_stats[aid]["failed"] += 1
    
    if not pattern_stats:
        return "*No structural assertions evaluated.*"
    
    lines = []
    lines.append("| Assertion | Pass | Fail | Rate |")
    lines.append("|-----------|------|------|------|")
    
    for aid in sorted(pattern_stats.keys()):
        stats = pattern_stats[aid]
        total = stats["passed"] + stats["failed"]
        rate = stats["passed"] / total * 100 if total > 0 else 0
        emoji = "‚úÖ" if rate >= 80 else "‚ö†Ô∏è" if rate >= 50 else "‚ùå"
        lines.append(f"| {aid} | {stats['passed']} | {stats['failed']} | {emoji} {rate:.0f}% |")
    
    return "\n".join(lines)


def generate_grounding_analysis(evaluation_data: Dict) -> str:
    """Generate grounding assertions analysis."""
    results = evaluation_data.get("results", [])
    
    # Aggregate grounding results
    pattern_stats = {}
    hallucinations = []
    
    for result in results:
        for gr in result.get("grounding_results", []):
            aid = gr.get("assertion_id", "Unknown")
            if aid not in pattern_stats:
                pattern_stats[aid] = {"passed": 0, "failed": 0}
            
            if gr.get("passed"):
                pattern_stats[aid]["passed"] += 1
            else:
                pattern_stats[aid]["failed"] += 1
                # Track hallucinations
                for span in gr.get("supporting_spans", []):
                    if span.get("type") == "mismatch":
                        hallucinations.append({
                            "quality": result.get("quality_level"),
                            "assertion": aid,
                            "value": span.get("text", "")
                        })
    
    if not pattern_stats:
        return "*No grounding assertions evaluated.*"
    
    lines = []
    lines.append("| Assertion | Pass | Fail | Rate |")
    lines.append("|-----------|------|------|------|")
    
    for aid in sorted(pattern_stats.keys()):
        stats = pattern_stats[aid]
        total = stats["passed"] + stats["failed"]
        rate = stats["passed"] / total * 100 if total > 0 else 0
        emoji = "‚úÖ" if rate >= 80 else "‚ö†Ô∏è" if rate >= 50 else "‚ùå"
        lines.append(f"| {aid} | {stats['passed']} | {stats['failed']} | {emoji} {rate:.0f}% |")
    
    # Show sample hallucinations
    if hallucinations:
        lines.append("")
        lines.append("**Sample Hallucinations Detected:**")
        for h in hallucinations[:5]:  # Show up to 5
            lines.append(f"- [{h['quality']}] {h['assertion']}: `{h['value'][:50]}...`")
    
    return "\n".join(lines)


def generate_verdict_section(evaluation_data: Dict) -> str:
    """Generate verdict distribution section."""
    summary = evaluation_data.get("summary", {})
    by_verdict = summary.get("by_verdict", {})
    
    total = sum(by_verdict.values())
    if total == 0:
        return "*No verdicts recorded.*"
    
    lines = []
    lines.append("| Verdict | Count | Percentage |")
    lines.append("|---------|-------|------------|")
    
    verdict_emoji = {
        "pass": "‚úÖ",
        "fail_structure": "üîß",
        "fail_grounding": "‚ö†Ô∏è",
        "fail_both": "‚ùå"
    }
    
    for verdict in ["pass", "fail_structure", "fail_grounding", "fail_both"]:
        count = by_verdict.get(verdict, 0)
        pct = count / total * 100 if total > 0 else 0
        emoji = verdict_emoji.get(verdict, "‚ùì")
        lines.append(f"| {emoji} {verdict} | {count} | {pct:.0f}% |")
    
    # Add interpretation
    lines.append("")
    lines.append("**Interpretation:**")
    lines.append("- ‚úÖ **pass**: Plan has good structure AND is factually accurate")
    lines.append("- üîß **fail_structure**: Plan is missing required elements")
    lines.append("- ‚ö†Ô∏è **fail_grounding**: Plan has good structure but contains hallucinations")
    lines.append("- ‚ùå **fail_both**: Plan is both incomplete and inaccurate")
    
    return "\n".join(lines)


def generate_detailed_results(evaluation_data: Dict) -> str:
    """Generate detailed results for each plan."""
    results = evaluation_data.get("results", [])
    
    if not results:
        return "*No detailed results available.*"
    
    lines = []
    
    for i, result in enumerate(results[:10]):  # Limit to first 10 for readability
        quality = result.get("quality_level", "unknown")
        scenario = result.get("scenario_id", "unknown")
        s_score = result.get("structural_score", 0) * 100
        g_score = result.get("grounding_score", 0) * 100
        verdict = result.get("overall_verdict", "unknown")
        
        verdict_emoji = {
            "pass": "‚úÖ",
            "fail_structure": "üîß",
            "fail_grounding": "‚ö†Ô∏è",
            "fail_both": "‚ùå"
        }.get(verdict, "‚ùì")
        
        lines.append(f"### Plan {i+1}: {scenario} ({quality.capitalize()})")
        lines.append("")
        lines.append(f"- **Structural Score:** {s_score:.0f}%")
        lines.append(f"- **Grounding Score:** {g_score:.0f}%")
        lines.append(f"- **Verdict:** {verdict_emoji} {verdict}")
        lines.append("")
        
        # Show failed assertions
        failed_structural = [r for r in result.get("structural_results", []) if not r.get("passed")]
        failed_grounding = [r for r in result.get("grounding_results", []) if not r.get("passed")]
        
        if failed_structural:
            lines.append("**Failed Structural Assertions:**")
            for fs in failed_structural[:3]:
                lines.append(f"- {fs.get('assertion_id')}: {fs.get('explanation', 'No explanation')[:80]}")
            lines.append("")
        
        if failed_grounding:
            lines.append("**Failed Grounding Assertions:**")
            for fg in failed_grounding[:3]:
                lines.append(f"- {fg.get('assertion_id')}: {fg.get('explanation', 'No explanation')[:80]}")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    if len(results) > 10:
        lines.append(f"*... and {len(results) - 10} more results (see JSON output for complete data)*")
    
    return "\n".join(lines)


def generate_insights_section(evaluation_data: Dict) -> str:
    """Generate insights and recommendations."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    
    lines = []
    
    # Framework effectiveness
    lines.append("### Framework Effectiveness")
    lines.append("")
    
    # Check if quality levels were properly differentiated
    perfect_s = by_quality.get("perfect", {}).get("avg_structural", 0)
    medium_s = by_quality.get("medium", {}).get("avg_structural", 0)
    low_s = by_quality.get("low", {}).get("avg_structural", 0)
    
    if perfect_s > medium_s > low_s:
        lines.append("‚úÖ **Quality Differentiation:** The framework correctly ranked quality levels (Perfect > Medium > Low)")
    else:
        lines.append("‚ö†Ô∏è **Quality Differentiation:** Quality levels may not be clearly differentiated")
    
    lines.append("")
    
    # Structural vs Grounding balance
    lines.append("### Structural vs Grounding Insights")
    lines.append("")
    
    for quality in ["perfect", "medium", "low"]:
        if quality not in by_quality:
            continue
        stats = by_quality[quality]
        s_diff = abs(stats["avg_structural"] - stats["avg_grounding"])
        if s_diff > 0.2:
            lines.append(f"- **{quality.capitalize()}:** {s_diff*100:.0f}% gap between structural ({stats['avg_structural']*100:.0f}%) and grounding ({stats['avg_grounding']*100:.0f}%) scores")
    
    lines.append("")
    
    # Recommendations
    lines.append("### Recommendations")
    lines.append("")
    lines.append("1. **Structural assertions** should focus on PRESENCE checking only")
    lines.append("2. **Grounding assertions** should always reference specific source fields")
    lines.append("3. **S9 (Grounding Meta-Check)** should be computed as AND(G1-G5)")
    lines.append("4. Plans with high structural but low grounding scores indicate hallucinations")
    lines.append("5. Plans with low structural scores need more complete content")
    
    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Main
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_report(evaluation_data: Dict, scenarios_data: Dict = None, 
                    assertions_data: Dict = None, plans_data: Dict = None) -> str:
    """Generate the complete evaluation report."""
    summary = evaluation_data.get("summary", {})
    by_quality = summary.get("by_quality", {})
    results = evaluation_data.get("results", [])
    
    # Calculate overall metrics
    total_structural = sum(len(r.get("structural_results", [])) for r in results)
    total_grounding = sum(len(r.get("grounding_results", [])) for r in results)
    
    all_s_scores = [r.get("structural_score", 0) for r in results]
    all_g_scores = [r.get("grounding_score", 0) for r in results]
    
    overall_s = sum(all_s_scores) / len(all_s_scores) * 100 if all_s_scores else 0
    overall_g = sum(all_g_scores) / len(all_g_scores) * 100 if all_g_scores else 0
    
    pass_count = summary.get("by_verdict", {}).get("pass", 0)
    pass_rate = pass_count / len(results) * 100 if results else 0
    
    # Count scenarios
    total_scenarios = len(scenarios_data.get("scenarios", [])) if scenarios_data else 0
    
    # Generate report sections
    report = REPORT_TEMPLATE.format(
        generated_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        total_scenarios=total_scenarios,
        total_plans=len(results),
        total_assertions=total_structural + total_grounding,
        overall_structural=f"{overall_s:.0f}",
        overall_grounding=f"{overall_g:.0f}",
        pass_rate=f"{pass_rate:.0f}",
        executive_summary=generate_executive_summary(evaluation_data),
        scenarios_section=generate_scenarios_section(scenarios_data) if scenarios_data else "*Scenarios data not loaded.*",
        assertions_section=generate_assertions_section(assertions_data) if assertions_data else "*Assertions data not loaded.*",
        plans_section=generate_plans_section(plans_data) if plans_data else "*Plans data not loaded.*",
        quality_level_section=generate_quality_level_section(evaluation_data),
        structural_analysis=generate_structural_analysis(evaluation_data),
        grounding_analysis=generate_grounding_analysis(evaluation_data),
        verdict_section=generate_verdict_section(evaluation_data),
        detailed_results=generate_detailed_results(evaluation_data),
        insights_section=generate_insights_section(evaluation_data)
    )
    
    return report


def main():
    """Main entry point for report generation."""
    parser = argparse.ArgumentParser(description="Stage 5: Report Generation")
    parser.add_argument("--scenarios", type=str, default=SCENARIOS_FILE, help="Input scenarios file")
    parser.add_argument("--assertions", type=str, default=ASSERTIONS_FILE, help="Input assertions file")
    parser.add_argument("--plans", type=str, default=PLANS_FILE, help="Input plans file")
    parser.add_argument("--evaluation", type=str, default=EVALUATION_FILE, help="Input evaluation file")
    parser.add_argument("--output", type=str, default=REPORT_FILE, help="Output report file (markdown)")
    parser.add_argument("--json", action="store_true", help="Also output JSON summary")
    args = parser.parse_args()
    
    print("\nüìà Stage 5: Report Generation")
    print("=" * 60)
    
    # Load all input data
    scenarios_data = None
    assertions_data = None
    plans_data = None
    
    try:
        scenarios_data = load_json(args.scenarios)
        print(f"  üìã Loaded {len(scenarios_data.get('scenarios', []))} scenarios from {args.scenarios}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Could not load scenarios: {e}")
    
    try:
        assertions_data = load_json(args.assertions)
        print(f"  üìã Loaded {len(assertions_data.get('assertions', []))} assertion sets from {args.assertions}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Could not load assertions: {e}")
    
    try:
        plans_data = load_json(args.plans)
        print(f"  üìã Loaded {len(plans_data.get('plans', []))} plans from {args.plans}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Could not load plans: {e}")
    
    # Load evaluation data (required)
    evaluation_data = load_json(args.evaluation)
    print(f"  üìä Loaded evaluation data from {args.evaluation}")
    
    # Generate report
    report = generate_report(evaluation_data, scenarios_data, assertions_data, plans_data)
    
    # Save report
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"  üíæ Saved report: {args.output}")
    
    # Optionally save JSON summary
    if args.json:
        json_output = args.output.replace(".md", "_summary.json")
        summary = {
            "generated_at": datetime.now().isoformat(),
            "source": args.evaluation,
            **evaluation_data.get("summary", {})
        }
        save_json(summary, json_output)
    
    print(f"\n‚úÖ Stage 5 Complete")
    print(f"   Report: {args.output}")
    
    return report


if __name__ == "__main__":
    main()
