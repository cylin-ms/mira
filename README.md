# Mira - Assertion Quality Evaluation Tool

**Version:** 2.0 (November 2025)

> **âœ¨ Mira** is a tool for **judges to evaluate the quality of LLM-generated assertions**. Judges review each assertion to determine if it is well-grounded in the meeting context and appropriately evaluates workback plan quality. GPT-5 JJ evaluation results are provided as **hints** to help judges quickly identify supporting evidence and make informed decisions.

---

## ğŸ™ Acknowledgments

| Contributor | Contribution |
|-------------|--------------|
| **Weiwei Cui** | Meeting Context Data (LOD - LiveOak Data), assertion methodology documentation |
| **Kening Ren** | Workback Plan response generation, assertions with justification and sourceID |
| **Chin-Yew Lin** | Mira annotation tool, GPT-5 JJ evaluation experiment, documentation |

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Quick Start](#quick-start)
3. [Judge's Guide: Evaluating Assertions](#judges-guide-evaluating-assertions)
4. [Using GPT-5 Results as Hints](#using-gpt-5-results-as-hints)
5. [Meeting Context & Entity Cards](#meeting-context--entity-cards)
6. [Optional: Response Annotations](#optional-response-annotations)
7. [Entity Card Rendering](#entity-card-rendering)
8. [Annotation Workflow](#annotation-workflow)
9. [Data Formats](#data-formats)
10. [Project Structure](#project-structure)
11. [Scripts Reference](#scripts-reference)
12. [Recent Changes](#recent-changes-november-2025)

---

## Overview

### What is Mira?

Mira is designed for **judges to evaluate assertion quality**. Each assertion is a statement about what a good workback plan response should contain, grounded in the meeting context data.

**Your task as a judge:**
1. Review each assertion and its justification
2. Check if the assertion is well-grounded in the meeting context (using the sourceID link)
3. Determine if the assertion is a good quality assertion (correct, clear, verifiable)
4. Mark the assertion as correct or incorrect, with optional notes

### Data Sources

| Data | Created By | Description |
|------|------------|-------------|
| **Meeting Context (LOD)** | Weiwei Cui | Entities (Events, Files, Emails, Chats, Users) that provide context |
| **Workback Plan Response** | Kening Ren | LLM-generated response to user's request |
| **Assertions** | Kening Ren | Statements about what the response should contain, with justification and sourceID |
| **GPT-5 Evaluation** | Chin-Yew Lin | Automated evaluation results to help judges (used as hints) |

### How GPT-5 Results Help Judges

The GPT-5 JJ evaluation was conducted as an **experiment** to:
1. **Verify real-world applicability**: Test how assertions might be used to automatically evaluate workback plan quality
2. **Provide hints for judges**: Help judges quickly find supporting evidence in the response
3. **Speed up annotation**: Judges can use GPT-5's pass/fail status and evidence spans as starting points

> **Note:** GPT-5 results are **hints only**. Judges should make their own independent judgment about assertion quality.

### Dataset Statistics

- **103 meetings** with workback plan requests
- **1,395 assertions** total (~13.5 per meeting)
- **GPT-5 evaluation**: 96.2% pass rate (used as reference, not ground truth)
- **SourceID Navigation**: Click "ğŸ”— View Entity" to jump to referenced entities
- **Visual Highlighting**: Linked entities highlighted with green banner
- **Azure Key Vault Integration**: Direct links to user credentials in Test Tenant

---

## Quick Start

### Prerequisites

- Python 3.10+
- Microsoft corporate account (for Azure Key Vault access)

### Setup

```bash
git clone https://github.com/cylin-ms/AssertionGeneration.git
cd AssertionGeneration
python -m venv .venv
.venv\Scripts\activate  # On Windows
pip install -r requirements.txt
```

### Launch Mira

```bash
streamlit run mira.py
```

Open your browser to `http://localhost:8501`.

---

## Judge's Guide: Evaluating Assertions

This is the **primary task** for judges using Mira.

### What You're Evaluating

Each assertion is a statement like:
> "The response should include the project deadline of December 15th mentioned in the kickoff meeting."

Your job is to determine if this assertion is **well-formed and correctly grounded** in the meeting context.

### Step-by-Step Annotation Process

#### Step 1: Select a Meeting

1. Click on a meeting in the sidebar (ğŸ“• = Not Started, ğŸ“™ = Partially Done, ğŸ“— = Complete)
2. Enter your name in the "ğŸ‘¤ Judge" field at the top

#### Step 2: Review the Assertion

For each assertion card:

1. **Read the assertion text** - What is it claiming the response should contain?
2. **Check the level** - Is it Critical, Expected, or Aspirational?
3. **Review the justification** - Why was this assertion created?

#### Step 3: Verify Grounding in Meeting Context

1. **Look for the sourceID** - Each assertion links to a specific entity
2. **Click "ğŸ”— View Entity"** - This expands the Meeting Context and highlights the source
3. **Review the source entity** - Does the assertion correctly reference this data?
4. **Check if the assertion is accurate** - Is the claim factually correct based on the entity?

#### Step 4: Make Your Judgment

| Checkbox | Meaning |
|----------|---------|
| **âœ… This assertion is correct** | The assertion is well-grounded and accurate |
| **ğŸ¯ Confident in judgment** | You're sure about your decision |
| **ğŸ“‹ Judged** | You've finished reviewing this assertion |

#### Step 5: Add Notes (Optional)

- **Note**: Explain why an assertion is incorrect or problematic
- **Revision**: Suggest improved assertion text

### Completion Criteria

| Status | Icon | Meaning |
|--------|------|---------|
| **Fully Judged** | ğŸ“— | All assertions in the meeting have been reviewed |
| **Partially Judged** | ğŸ“™ | Some assertions reviewed, but not all |
| **Not Started** | ğŸ“• | No assertions reviewed yet |

### Saving Your Work

- **Auto-save**: Annotations save automatically
- **Manual save**: Click "ğŸ’¾ Save" anytime
- **Export**: Click "ğŸ“¤ Export" to create `docs/annotated_output.jsonl`

---

## Using GPT-5 Results as Hints

GPT-5 JJ was used to automatically evaluate assertions against the response. These results are provided as **hints** to help you work faster.

### How GPT-5 Results Appear

Each assertion shows a GPT-5 status in its header:
- **âœ…** = GPT-5 determined the assertion PASSED (response satisfies it)
- **âŒ** = GPT-5 determined the assertion FAILED (response doesn't satisfy it)

### What GPT-5 Provides

When you expand an assertion, you'll see:

1. **Pass/Fail Status**: GPT-5's automated judgment
2. **Explanation**: Why GPT-5 made this decision
3. **Supporting Spans**: Exact quotes from the response that GPT-5 identified as evidence
4. **Confidence Scores**: How confident GPT-5 is about each piece of evidence
5. **Section Attribution**: Which part of the response contains each span

### How to Use GPT-5 Hints

| GPT-5 Says | What You Should Do |
|------------|-------------------|
| âœ… PASSED | Use the supporting spans to quickly verify the assertion is good |
| âŒ FAILED | Check if GPT-5 is correct, or if the assertion is actually fine |
| High confidence spans | These are likely reliable evidence |
| Low confidence spans | Review these more carefully |

### Verifying or Rejecting GPT-5 Results

You can **verify or reject** GPT-5's findings:

1. **Explanation checkbox**: Uncheck if you disagree with GPT-5's reasoning
2. **Span checkboxes**: Uncheck individual evidence spans you think are wrong
3. Rejected items appear crossed out in the UI

> **Remember:** GPT-5 results are hints, not ground truth. Your judgment is what matters!

### GPT-5 Experiment Statistics

The GPT-5 evaluation was run on all 103 meetings:
- **1,395 assertions** evaluated
- **96.2% pass rate** (1,342 passed, 53 failed)
- **100% span coverage** (all assertions have supporting evidence)

---

## Meeting Context & Entity Cards

The Meeting Context contains all the source data used to generate assertions.

### Why Meeting Context Matters

Assertions reference specific entities via `sourceID`. To verify an assertion, you need to check the actual entity it references.

### Accessing Meeting Context

1. **Notice the blue banner**: "ğŸ“‹ Meeting Context Available"
2. **Click the expander**: "ğŸ“¥ View Meeting Context"
3. **Browse entities**: Each entity type has a rich card display

### Entity Card Types

| Entity Type | Icon | What It Shows |
|-------------|------|---------------|
| **Event** | ğŸ“… | Meeting subject, time, attendees, location |
| **File** | ğŸ“„ | Document name, content preview with tabs |
| **Chat** | ğŸ’¬ | Chat messages displayed as bubbles |
| **Email** | âœ‰ï¸ | Subject, recipients, body preview |
| **User** | ğŸ‘¤ | Name, avatar, Azure Key Vault link |
| **ChannelMessage** | ğŸ“¢ | Teams message with content |

### SourceID Linking

When you click "ğŸ”— View Entity" on an assertion:
1. The Meeting Context section auto-expands
2. The referenced entity is highlighted with a green banner
3. You can verify if the assertion correctly references the entity

---

## Optional: Response Annotations

Judges can **optionally** annotate the generated response itself. This is **not mandatory**.

### Per-Section Annotations

The response is parsed into sections. For each section:
1. Click "ğŸ“ Add annotation for this section"
2. Enter comments about that specific section

### Overall Response Comment

At the bottom, there's an "ğŸ“‹ Overall Response Annotation" box for general comments about the response quality.

> **Note:** Response annotations are optional and separate from assertion evaluation.

---

## Entity Card Rendering

Mira provides rich card rendering for different entity types found in the meeting context.

### Chat Entity Cards ğŸ’¬

Displays chat conversations with a two-column layout:

| Left Column | Right Column |
|-------------|--------------|
| Chat metadata (ID, topic) | Message bubbles |
| Member count | Sender, content, timestamp |

Each message is rendered as a styled bubble with:
- ğŸ‘¤ Sender name in blue
- Message content
- Timestamp aligned right

### Email Entity Cards âœ‰ï¸

Displays email with metadata and body preview:

| Left Column | Right Column |
|-------------|--------------|
| Subject, From, Date | Email body |
| To/Cc recipients | Rendered content |

### File Entity Cards ğŸ“„

Displays document files with content preview:

| Left Column | Right Column |
|-------------|--------------|
| File name, type, size | Content tabs |
| Metadata table | ğŸ“„ Preview / â„¹ï¸ Raw Source |

### User Entity Cards ğŸ‘¤

Displays user information with avatar:

- Visual avatar with initials
- Display name and user ID
- Mail nickname
- ğŸ”— Azure Key Vault link for Test Tenant access

### ChannelMessage Cards ğŸ“¢

Displays Teams channel messages:

| Left Column | Right Column |
|-------------|--------------|
| Message info | Content |
| Sender, timestamp | Full message body |

### Generic Entity Card

For any entity type with a `Content` or `Body` field, automatically displays the content.

---

## Annotation Workflow

This section describes the complete annotation workflow using Mira for **evaluating assertion quality**.

### Step 1: Launch Mira and Set Up

```bash
streamlit run mira.py
```

1. **Enter Your Name**: Type your name in the "ğŸ‘¤ Judge" field
2. **Select Filter** (optional): Use "ğŸ” Filter" to focus on specific meeting types
3. **Select a Meeting**: Click on a meeting in the sidebar (ğŸ“• = Not Started)

### Step 2: Review Meeting Context (Important!)

> **ğŸ“‹ Note:** The Meeting Context section contains all the source data that was used to generate the assertions. Reviewing this context is **essential** for verifying assertion quality.

1. **Notice the Banner**: A prominent blue banner "ğŸ“‹ Meeting Context Available" alerts you to the available context
2. **Expand Meeting Context**: Click "ğŸ“¥ View Meeting Context (Entities, Organizer, Source Data)"
3. **Review Meeting Organizer**: 
   - See the organizer's name, ID, and avatar
   - Click the name to access Azure Key Vault credentials (for Test Tenant login)
4. **Browse Entity Cards**: These are the source entities referenced by assertions
   - ğŸ“… **Event cards**: Meeting details, attendees, time/location
   - ğŸ’¬ **Chat cards**: Message history displayed as styled bubbles
   - âœ‰ï¸ **Email cards**: Subject, recipients, and full body preview
   - ğŸ“„ **File cards**: Document content with Preview/Raw tabs
   - ğŸ‘¤ **User cards**: User info with Azure Key Vault link
5. **Check SourceID Links**: When you click "ğŸ”— View Entity" on an assertion, it will:
   - Auto-expand the Meeting Context section
   - Highlight the linked entity with a green banner
   - Help you verify if the assertion is correctly grounded in source data

### Step 3: Evaluate Each Assertion (Primary Task)

**This is the main task.** For each assertion, you need to determine if it's a high-quality assertion:

1. **Expand the Assertion**: Click to see full details
2. **Use GPT-5 Results as Hints** (if available):
   - Look for âœ… (passed) or âŒ (failed) icons in the header
   - Review the color-coded supporting spans
   - These are **hints to assist your decision**, not final verdicts
   - You can verify or reject GPT-5's findings using the checkboxes
3. **Review the Assertion Content**:
   - Is the assertion clear and specific?
   - Is it properly grounded in the source data (check SourceID)?
   - Is the level (critical/expected/aspirational) appropriate?
   - Is the justification reasonable?
4. **Make Your Judgment**:
   - Toggle "âœ… This assertion is correct" based on your evaluation
   - Check "ğŸ¯ Confident in judgment" if you're sure
   - Check "ğŸ“‹ Judged" to mark as reviewed
5. **Add Notes/Revision** (optional): Explain issues or suggest improvements

### Step 4: Annotate the Response (Optional)

> **Note:** Response annotation is **optional**. The primary goal is evaluating assertions.

If you choose to also evaluate the response:

1. **Read Section Content**: Review each section of the LLM response
2. **Add Section Annotations**: Click "ğŸ“ Add annotation for this section"
3. **Add Overall Comment**: Use "ğŸ“‹ Overall Response Annotation" at the bottom

### Step 5: Track Progress & Export

- **Progress Bar**: Shows assertion completion percentage
- **Status Indicators**: ğŸ“— Complete | ğŸ“™ Partial | ğŸ“• Not Started
- **Auto-Save**: Annotations save to `docs/annotations_temp.json`
- **Export**: Click "ğŸ“¤ Export" to create `docs/annotated_output.jsonl`

### Annotation Data Saved

For each assertion, the following data is captured:

| Field | Description |
|-------|-------------|
| `is_good` | Whether the assertion is a high-quality assertion (true/false) |
| `is_confident` | Whether you're confident in your judgment |
| `is_judged` | Whether the assertion has been reviewed |
| `note` | Optional explanation or comments |
| `revision` | Optional suggested improvement |
| `gpt5_verification` | GPT-5 explanation/span verification status |

### Best Practices for Assertion Evaluation

1. **Judge all assertions** in a meeting to mark it as "Fully Judged"
2. **Use the Meeting Context** to verify assertion grounding
3. **Use GPT-5 results as hints** - they can help you quickly identify potential issues
4. **Don't blindly trust GPT-5** - always verify with source data
5. **Add notes** for incorrect assertions to explain why they fail
6. **Save frequently** - click "ğŸ’¾ Save" or wait for auto-save
7. **Export before closing** - click "ğŸ“¤ Export"

---

## Data Formats

### Assertion Output Format (11_25_output.jsonl)

```json
{
  "utterance": "Help me make a workback plan for...",
  "response": "Here's your workback plan...",
  "assertions": [
    {
      "text": "The response should include...",
      "level": "critical|expected|aspirational",
      "justification": {
        "reason": "Explanation of why this assertion matters",
        "sourceID": "entity-uuid-reference"
      }
    }
  ]
}
```

**Note:** The app supports both new format (`justification`/`sourceID`) and legacy format (`reasoning`/`source`).

### WBP Converted Assertion Format (assertions_converted_full.jsonl)

This format is used by **Mira 2.0** for viewing WBP-framework assertions with dimension/layer/weight classification:

```json
{
  "user": {
    "id": "lod_username",
    "displayName": "Full Name",
    "mailNickName": "lod_username",
    "url": "https://ms.portal.azure.com/..."
  },
  "utterance": "Help me make a workback plan for...",
  "response": "Here's your workback plan...",
  "assertions": [
    {
      "assertion_id": "A0000_S1",
      "parent_assertion_id": null,
      "text": "The response should state the meeting [SUBJECT], [DATE/TIME]...",
      "level": "critical|expected|aspirational",
      "dimension": "S1",
      "dimension_name": "Meeting Details",
      "layer": "structural",
      "weight": 3,
      "sourceID": "entity-uuid-reference",
      "original_text": "Original Kening assertion text...",
      "rationale": {
        "mapping_reason": "Why this maps to S1",
        "quality_notes": "Assessment notes"
      },
      "quality_assessment": {
        "clarity": "high",
        "specificity": "high"
      },
      "conversion_method": "heuristic"
    },
    {
      "assertion_id": "A0000_G2_0",
      "parent_assertion_id": "A0000_S1",
      "text": "All people mentioned must exist in {source.ATTENDEES}",
      "level": "critical",
      "dimension": "G2",
      "dimension_name": "Attendee Grounding",
      "layer": "grounding",
      "weight": 3,
      "sourceID": "",
      "original_text": "Original Kening assertion text...",
      "rationale": {
        "mapping_reason": "Generated from S1 (Meeting Details) via S_TO_G_MAP",
        "parent_dimension": "S1",
        "parent_dimension_name": "Meeting Details"
      },
      "quality_assessment": { "is_well_formed": true, "is_testable": true },
      "conversion_method": "heuristic_s_to_g"
    }
  ]
}
```

**Key fields:**
- `assertion_id`: Unique identifier for the assertion (e.g., `A0000_S1`, `A0000_G2_0`)
- `parent_assertion_id`: Links G assertions to their source S assertion (`null` for S assertions)
- `dimension`: WBP dimension code (S1-S19, G1-G8)
- `dimension_name`: Human-readable dimension name
- `layer`: "structural" or "grounding"
- `weight`: Importance weight (1-3)
- `original_text`: Kening's original assertion text
- `conversion_method`: How the assertion was converted (`gpt5`, `heuristic`, or `heuristic_s_to_g`)

**S+G Adjacency:** Structural (S) assertions are immediately followed by their derived Grounding (G) assertions, keeping related assertions grouped together.

### Context File Format (LOD_1121.WithUserUrl.jsonl)

```json
{
  "UTTERANCE": {"text": "User request..."},
  "USER": {
    "id": "lod_username",
    "displayName": "Full Name",
    "mailNickName": "lod_username",
    "url": "https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/..."
  },
  "ENTITIES_TO_USE": [
    {"type": "Event", "EventId": "...", "Subject": "..."},
    {"type": "File", "FileId": "...", "FileName": "...", "Content": "..."},
    {"type": "Chat", "ChatId": "...", "ChatMessages": [...]},
    {"type": "Email", "EmailId": "...", "Body": "..."}
  ]
}
```

### GPT-5 Scores Format (assertion_scores.json)

```json
{
  "timestamp": "2025-11-26T22:35:47",
  "num_samples": 103,
  "overall_stats": {
    "total_assertions": 1395,
    "passed_assertions": 1342,
    "pass_rate": 96.2
  },
  "meetings": [
    {
      "utterance": "...",
      "assertion_results": [
        {
          "assertion_text": "...",
          "level": "critical",
          "passed": true,
          "explanation": "...",
          "supporting_spans": [...]
        }
      ]
    }
  ]
}
```

### Export Format (annotated_output.jsonl)

```json
{
  "utterance": "...",
  "response": "...",
  "assertions": [...],
  "annotations": [
    {
      "assertion_index": 0,
      "original_text": "...",
      "is_good": true,
      "is_confident": true,
      "is_judged": true,
      "note": "...",
      "revised_text": null,
      "gpt5_verification": {...}
    }
  ],
  "response_annotations": {
    "section_0": "...",
    "overall": "..."
  },
  "judge": "Your Name"
}
```

### Context Files

| File | Description |
|------|-------------|
| `docs/LOD_1121.WithUserUrl.jsonl` | **Current** - with Azure Key Vault URLs |
| `docs/LOD_1125.jsonl` | Previous version (Nov 25, 2025) |
| `docs/LOD_1121.jsonl` | Original version (Nov 21, 2025) |

---

## Project Structure

```
AssertionGeneration/
â”œâ”€â”€ mira.py                      # ğŸ¯ Main annotation tool (Streamlit)
â”œâ”€â”€ evaluate_assertions_gpt5.py  # ğŸ¤– GPT-5 JJ automated evaluation
â”œâ”€â”€ compute_assertion_matches.py # Find evidence for assertions
â”œâ”€â”€ score_assertions.py          # Automated PASS/FAIL evaluation
â”œâ”€â”€ show_assertion_details.py    # Generate HTML reports
â”œâ”€â”€ check_sourceid_recovery.py   # Verify SourceID mappings
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ 11_25_output.jsonl           # Assertions (103 meetings)
â”‚   â”œâ”€â”€ assertion_scores.json        # GPT-5 evaluation results
â”‚   â”œâ”€â”€ LOD_1121.WithUserUrl.jsonl   # ğŸ“Œ Context with Azure Key Vault URLs
â”‚   â”œâ”€â”€ LOD_1125.jsonl               # Context file (99% SourceID match)
â”‚   â”œâ”€â”€ LOD_1121.jsonl               # Original context file
â”‚   â”œâ”€â”€ annotations_temp.json        # Auto-saved annotations
â”‚   â”œâ”€â”€ annotated_output.jsonl       # Exported annotations
â”‚   â”œâ”€â”€ DATA_GENERATION.md           # Dataset creation docs
â”‚   â”œâ”€â”€ deriving_assertions_workback_plan.md  # Assertion methodology
â”‚   â””â”€â”€ screenshots/                 # UI screenshots
â””â”€â”€ README.md
```

---

## Scripts Reference

### Primary Scripts

| Script | Description | Usage |
|--------|-------------|-------|
| `mira.py` | Main annotation tool | `streamlit run mira.py` |
| `evaluate_assertions_gpt5.py` | GPT-5 automated evaluation | `python evaluate_assertions_gpt5.py` |

### Evaluation Scripts

| Script | Description |
|--------|-------------|
| `evaluate_assertions_gpt5.py` | Full GPT-5 evaluation with span extraction |
| `score_assertions.py` | Simple PASS/FAIL scoring |
| `compute_assertion_matches.py` | Find evidence for assertions |

### Utility Scripts

| Script | Description |
|--------|-------------|
| `show_assertion_details.py` | Generate HTML reports |
| `show_assertion_html.py` | Additional HTML output |
| `check_sourceid_recovery.py` | Verify SourceID mappings |
| `check_models.py` | Check available models |

### Command Reference

```powershell
# Launch Mira annotation tool
streamlit run mira.py

# Run GPT-5 evaluation on all meetings
python evaluate_assertions_gpt5.py --start 0 --end 103

# Evaluate single meeting
python evaluate_assertions_gpt5.py --meeting-num 7

# Force re-evaluate all
python evaluate_assertions_gpt5.py --force

# Compute assertion matches
python compute_assertion_matches.py --use-jj --jj-delay 3 --input docs/11_25_output.jsonl --output docs/output_with_matches.jsonl

# Generate HTML report
python show_assertion_details.py --input docs/output_with_matches.jsonl --output docs/assertion_details.html --open
```

---

## Recent Changes (November 2025)

### v2.0 - GPT-5 JJ Integration & Entity Cards (Nov 26, 2025)

**Major Features:**

ğŸ¤– **GPT-5 JJ Automated Evaluation**
- Full evaluation of 103 meetings, 1,395 assertions (96.2% pass rate)
- Supporting span extraction with confidence scoring
- Section attribution for each evidence span
- Visual highlighting in Mira UI
- Annotator verification checkboxes for GPT-5 results

ğŸ“§ **Rich Entity Card Rendering**
- `render_chat_card()`: Chat entities with message bubbles
- `render_email_card()`: Email with body preview
- `render_channel_message_card()`: Teams messages with content
- `render_user_card()`: User avatar with Azure Key Vault link
- `render_file_card()`: File metadata + content with tabs
- `render_generic_card()`: Fallback for Content/Body fields

ğŸ¨ **UI Improvements**
- Meeting title styled with purple gradient (matches badge)
- File renamed from `visualize_output.py` to `mira.py`
- Entity type detection fix (actual_entity_type from data)
- MailNickName added to User entity indexing

### v1.5 - Command Center UI (Nov 25, 2025)

- ğŸ›ï¸ Consolidated command center at top
- ğŸ‘¤ Modern user card with avatar
- ğŸ“ Per-section response annotations
- ğŸ“Š Enhanced progress tracking
- ğŸ” Smart filtering by annotation status

### v1.0 - Initial Release (Nov 24, 2025)

- Basic assertion visualization
- Entity linking with SourceID navigation
- Azure Key Vault integration
- Annotation export functionality

---

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## License

Microsoft Internal Use Only